# extraction_service.py
import os
from pathlib import Path
import json
from collections import Counter
from concurrent.futures import ThreadPoolExecutor
import re

# Simple regex-based tokenizer (no NLTK dependency for Python 3.14 compatibility)
def simple_tokenize(text):
    """Simple word tokenizer using regex"""
    return re.findall(r'\b\w+\b', text.lower())


# -------------------------------
# Extraction depuis un fichier texte
# -------------------------------
def extract_from_text(file_path):
    """
    Lit un fichier texte, tokenize et calcule les statistiques compl√®tes
    
    Arguments :
    - file_path : chemin vers le fichier texte
    
    Retour :
    - tuple (nom fichier sans extension, dictionnaire avec les statistiques)
    """
    try:
        # Lecture du texte original et nettoy√©
        with open(file_path, "r", encoding="utf-8") as f:
            clean_text = f.read()

        # On retrouve le chemin du texte original
        original_file = Path("data/processed/raw_texts") / file_path.name
        if original_file.exists():
            with open(original_file, "r", encoding="utf-8") as f:
                original_text = f.read()
        else:
            original_text = clean_text  # Fallback si on ne trouve pas l'original

        # Stats sur le texte original
        tokens_before = simple_tokenize(original_text)
        total_tokens_before = len(tokens_before)
        char_count_before = len(original_text)

        # Stats sur le texte nettoy√©
        tokens_after = simple_tokenize(clean_text)
        total_tokens_after = len(tokens_after)
        char_count_after = len(clean_text)

        # Fr√©quence des mots (tous les mots)
        word_freq = Counter(tokens_after)
        all_words = [(word, count) for word, count in word_freq.items()]

        # Bigrammes complets
        bigrams = list(zip(tokens_after, tokens_after[1:]))
        bigram_freq = Counter([" ".join(bg) for bg in bigrams])
        all_bigrams = [(bigram, count) for bigram, count in bigram_freq.items()]

        # Nom du fichier avec extension
        original_name = file_path.name

        return original_name, {
            "context": clean_text,  # Texte nettoy√©
            "total_tokens_before": total_tokens_before,
            "total_tokens_after": total_tokens_after,
            "char_count_before": char_count_before,
            "char_count_after": char_count_after,
            "words": all_words,  # Tous les mots avec leur fr√©quence
            "bigrams": all_bigrams  # Tous les bigrammes avec leur fr√©quence
        }

    except Exception as e:
        # En cas d'erreur, on retourne un dictionnaire vide pour ce fichier
        print(f"‚ùå Erreur extraction {file_path.name}: {e}")
        return file_path.name, {}


# -------------------------------
# Extraction globale du corpus + fusion JSON
# -------------------------------
def extract_corpus(
    input_dir="data/processed/clean_texts",
    output_file="data/processed/metadata.json",
    max_workers=4
):
    """
    Parcourt un dossier de textes normalis√©s, calcule les statistiques par fichier,
    et fusionne les r√©sultats dans un fichier JSON.
    
    Arguments :
    - input_dir : dossier contenant les fichiers .txt normalis√©s
    - output_file : fichier JSON pour stocker les m√©tadonn√©es et statistiques
    - max_workers : nombre de threads pour l'extraction parall√®le
    """
    input_path = Path(input_dir)
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Lecture du JSON existant (si pr√©sent) pour fusionner avec les nouvelles donn√©es
    if output_path.exists():
        try:
            with open(output_path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
            print(f"üîÑ Fichier JSON existant charg√© ({len(existing_data)} fichiers).")
        except Exception:
            print("‚ö†Ô∏è Erreur lecture JSON existant ‚Äî r√©initialisation.")
            existing_data = {}
    else:
        existing_data = {}

    # Liste des fichiers texte √† traiter
    files = [f for f in input_path.glob("*.txt")]
    results = {}

    # Traitement parall√®le avec ThreadPoolExecutor pour acc√©l√©rer l'extraction
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        for name, data in executor.map(lambda f: extract_from_text(f), files):
            results[name] = data

    # Fusion : mise √† jour ou ajout des r√©sultats dans le JSON existant
    for name, data in results.items():
        if name in existing_data:
            existing_data[name].update(data)
        else:
            existing_data[name] = data

    # Sauvegarde des r√©sultats fusionn√©s
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=4)

    print(f"\n‚úÖ R√©sultats fusionn√©s sauvegard√©s dans {output_file}\n")

    # Affichage console pour suivi rapide
    for name, data in results.items():
        print(f"üìÑ {name} : {data.get('total_tokens', 0)} tokens")
        print("Top mots :", data.get("top_words", []))
        print("Top bigrammes :", data.get("top_bigrams", []))
        print("-" * 60)

    return existing_data


# -------------------------------
# Ex√©cution directe
# -------------------------------
if __name__ == "__main__":
    # Appel principal pour extraire le corpus et g√©n√©rer JSON
    extract_corpus()
